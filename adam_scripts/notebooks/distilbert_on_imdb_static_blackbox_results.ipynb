{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ef8426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import random\n",
    "import string\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, '/Users/awgol/code/syne-tune/')\n",
    "\n",
    "import syne_tune\n",
    "importlib.reload(syne_tune)\n",
    "\n",
    "import syne_tune.experiments\n",
    "importlib.reload(syne_tune.experiments)\n",
    "\n",
    "from syne_tune.util import experiment_path, s3_experiment_path\n",
    "importlib.reload(syne_tune.util)\n",
    "\n",
    "import adam_scripts.utils\n",
    "importlib.reload(adam_scripts.utils)\n",
    "\n",
    "import pygmo\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import boto3\n",
    "\n",
    "from syne_tune.backend.sagemaker_backend.instance_info import InstanceInfos\n",
    "instance_info = InstanceInfos()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import QuantileTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a4d4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3_client_sagemaker = boto3.client('sagemaker')\n",
    "boto3_resource_cloudwatch = boto3.resource('cloudwatch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af873aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPEED_SYNE_TUNE_JOB_NAMES = (\n",
    "    'speed-bs-it-nw-new-2022-02-21-18-05-01-921',\n",
    ")\n",
    "\n",
    "BLACKBOX_SPEED_S3_PATH = (\n",
    "    's3://mnemosyne-team-bucket/dataset/'\n",
    "    'hf-distilbert-on-imdb-static-blackbox/hf-distilbert-on-imdb-static-blackbox-speed.csv.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd4ce27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(BLACKBOX_SPEED_S3_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247aa08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syne Tune's st_worker_time and st_tuner_cost do not account for either:\n",
    "# a) the EC2 instance startup overhead time (customers don't pay for this)\n",
    "# b) the time at the beginning of the running of the script (starting the script, setting up the dataloader etc)\n",
    "# We assume an idealized scenario where we have each instance type already running and with dataloader prepared,\n",
    "# such that they're all ready to start our jobs. Hence we can omit both effects a) and b).\n",
    "# However, we still assume the attempts that fail due to OOM, incur the cost equivalent to SCRIPT_SETUP_OVERHEAD_TIME.\n",
    "INSTANCE_STARTUP_OVERHEAD_TIME = 0  # in seconds\n",
    "# This was estimated as the median of \"BillableTimeInSeconds-st_worker_time\" for runs of a given training script\n",
    "# (gluonts-on-electricity or hugging-distil-bert-finetunes-on-imdb).\n",
    "# For gluonts on 'deepar-speed-bs-32-2022-04-21-14-48-44-131': SCRIPT_SETUP_OVERHEAD_TIME = 65\n",
    "# For distill-bert-on-imdb on 'loss-lr-wd-bs-2-2022-02-07-23-13-30-781': SCRIPT_SETUP_OVERHEAD_TIME = 410\n",
    "SCRIPT_SETUP_OVERHEAD_TIME = 0  # in seconds\n",
    "\n",
    "# The speed benchmarking experiments were run for max_run=5min timeout setting of the HuggingFace estimator.\n",
    "# If we are more efficient about software engineering of the solution we could run only a few batches\n",
    "# and do it cheaper.\n",
    "ST_WORKER_TIME_DISCOUNT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d927c84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates resulting from what I understand is the following issue\n",
    "# https://github.com/awslabs/syne-tune/issues/214\n",
    "temp = df.groupby(['trial_id', 'step']).loss.count().reset_index()\n",
    "trial_ids_to_be_deleted = temp[temp.loss>1].trial_id.unique() \n",
    "trial_ids_to_be_deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2506d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df.index[df['trial_id'].isin(trial_ids_to_be_deleted)], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99118a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# speed-bs-it-nw-new-2022-02-21-18-05-01-921-82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b3ac7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute time per samples\n",
    "dfg = df.groupby(['trial_id'])\n",
    "\n",
    "number_of_samples_processed = (\n",
    "  dfg.step.max() * \n",
    "  dfg.config_per_device_train_batch_size.max() * \n",
    "  dfg.config_st_instance_type.max().map(lambda x: instance_info(x).num_gpu))\n",
    "\n",
    "samples_processed_per_second = number_of_samples_processed / dfg.st_worker_time.max()\n",
    "time_per_sample = dfg.st_worker_time.max() / number_of_samples_processed\n",
    "cost_per_sample = time_per_sample * dfg.config_st_instance_type.max().map(lambda x: instance_info(x).cost_per_hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bce145a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The records that have only 100 steps record, which means that their time_per_sample cannot be estimated \n",
    "# by taking the difference between the first and last record.\n",
    "# We do it this way such that the results are free of the bias reflected in st_tuner_time due to the \n",
    "# script_startup_overhead.\n",
    "\n",
    "# These are trial_ids corresponding to the trials with only 100 steps records\n",
    "dfg.step.max().index[np.argwhere((dfg.step.max()==100).to_numpy()).reshape(-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7f011b",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = pd.concat([\n",
    "    samples_processed_per_second, \n",
    "    time_per_sample,\n",
    "    cost_per_sample,\n",
    "    dfg.config_seed.max(),\n",
    "    dfg.st_worker_cost.max(), \n",
    "    dfg.config_st_instance_type.max(), \n",
    "    dfg.config_per_device_train_batch_size.max(),\n",
    "    dfg.config_dataloader_num_workers.max(),\n",
    "], axis=1)\n",
    "columns = ['samples_processed_per_second', 'time_per_sample', 'cost_per_sample']\n",
    "b.columns = columns + list(b.columns)[len(columns):]\n",
    "\n",
    "b = (\n",
    "    b.groupby([\n",
    "        'config_st_instance_type', \n",
    "        'config_per_device_train_batch_size', \n",
    "        'config_dataloader_num_workers'\n",
    "    ]).agg({\n",
    "        'samples_processed_per_second': 'mean',\n",
    "        'time_per_sample': 'mean',\n",
    "        'cost_per_sample': 'mean',\n",
    "        'st_worker_cost': 'mean',\n",
    "}).reset_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353cd9fa",
   "metadata": {},
   "source": [
    "Add failed runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c742f4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# per_device_train_batch_size_list = [4, 8, 16, 24, 32, 40, 48]\n",
    "per_device_train_batch_size_list = [8, 16, 24, 32, 40, 48]\n",
    "dataloader_num_workers_list = [0, 1]\n",
    "instance_types = [\n",
    "        'ml.p3.2xlarge',\n",
    "        'ml.p2.xlarge',\n",
    "        'ml.p2.8xlarge',\n",
    "        'ml.g4dn.xlarge',\n",
    "        'ml.g4dn.2xlarge',\n",
    "        'ml.g4dn.4xlarge',\n",
    "        'ml.g4dn.8xlarge',\n",
    "        'ml.g4dn.12xlarge',\n",
    "        'ml.g5.xlarge',\n",
    "        'ml.g5.2xlarge',\n",
    "        'ml.g5.4xlarge',\n",
    "        'ml.g5.8xlarge',\n",
    "        'ml.g5.12xlarge',\n",
    "        'ml.g5.24xlarge',\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5868a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\n",
    "    \"config_st_instance_type\", \n",
    "    \"config_per_device_train_batch_size\", \n",
    "    \"config_dataloader_num_workers\"\n",
    "]\n",
    "\n",
    "mind = pd.MultiIndex.from_product(\n",
    "    [instance_types, per_device_train_batch_size_list, dataloader_num_workers_list], \n",
    "    names=names)\n",
    "c = b.set_index(names).reindex(mind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a690af",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62c0a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(instance_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab200782",
   "metadata": {},
   "outputs": [],
   "source": [
    "2*14*6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00ff956",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in c.st_worker_cost.isna().loc[lambda x: x==True].index:\n",
    "    c.loc[idx].samples_processed_per_second = 0.0\n",
    "    c.loc[idx].time_per_sample = 1.\n",
    "    c.loc[idx].cost_per_sample = 1.\n",
    "    c.loc[idx].config_seed = 1\n",
    "    c.loc[idx].st_worker_cost = (instance_info(idx[0]).cost_per_hour /\n",
    "                                 60. / 60. * SCRIPT_SETUP_OVERHEAD_TIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410ad2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = c.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc788a1",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32b6a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = c\n",
    "y_all_points = np.stack([\n",
    "    temp.time_per_sample,\n",
    "    temp.cost_per_sample\n",
    "], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e49063",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_all_points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70259219",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "normalizer = QuantileTransformer(output_distribution='uniform', n_quantiles=len(y_all_points))\n",
    "normalizer.fit(y_all_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0921963",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_point = (1.+1e-15, 1.+1e-15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9ce449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hv(inputs_unnorm):\n",
    "    inputs_unnorm_vec = np.stack([inputs_unnorm.time_per_sample, inputs_unnorm.cost_per_sample], axis=-1)\n",
    "    inputs_norm_vec = normalizer.transform(inputs_unnorm_vec)\n",
    "    hv = pygmo.hypervolume(inputs_norm_vec.tolist()).compute(ref_point)\n",
    "    return hv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f34c36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_hv = compute_hv(c)\n",
    "true_hv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c312d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/32791911/fast-calculation-of-pareto-front-in-python\n",
    "\n",
    "# Faster than is_pareto_efficient_simple, but less readable.\n",
    "def is_pareto_efficient(costs, return_mask = True):\n",
    "    \"\"\"\n",
    "    Find the pareto-efficient points\n",
    "    :param costs: An (n_points, n_costs) array\n",
    "    :param return_mask: True to return a mask\n",
    "    :return: An array of indices of pareto-efficient points.\n",
    "        If return_mask is True, this will be an (n_points, ) boolean array\n",
    "        Otherwise it will be a (n_efficient_points, ) integer array of indices.\n",
    "    \"\"\"\n",
    "    is_efficient = np.arange(costs.shape[0])\n",
    "    n_points = costs.shape[0]\n",
    "    next_point_index = 0  # Next index in the is_efficient array to search for\n",
    "    while next_point_index<len(costs):\n",
    "        nondominated_point_mask = np.any(costs<costs[next_point_index], axis=1)\n",
    "        nondominated_point_mask[next_point_index] = True\n",
    "        is_efficient = is_efficient[nondominated_point_mask]  # Remove dominated points\n",
    "        costs = costs[nondominated_point_mask]\n",
    "        next_point_index = np.sum(nondominated_point_mask[:next_point_index])+1\n",
    "    if return_mask:\n",
    "        is_efficient_mask = np.zeros(n_points, dtype = bool)\n",
    "        is_efficient_mask[is_efficient] = True\n",
    "        return is_efficient_mask\n",
    "    else:\n",
    "        return is_efficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34807da6",
   "metadata": {},
   "source": [
    "### Pareto optimal points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca33f4c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inputs_unnorm = c\n",
    "inputs_unnorm_vec = np.stack([inputs_unnorm.time_per_sample, inputs_unnorm.cost_per_sample], axis=-1)\n",
    "inputs_norm_vec = normalizer.transform(inputs_unnorm_vec)\n",
    "pygmo.hypervolume(inputs_norm_vec.tolist()).get_points()\n",
    "\n",
    "inputs_unnorm[is_pareto_efficient(inputs_norm_vec)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377784da",
   "metadata": {},
   "source": [
    "### Reused code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c7c10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dict()\n",
    "costs = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2d50c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hvs_and_cost(temp, scenario_label, K):\n",
    "    hvs_total = []\n",
    "    costs_total = []\n",
    "    for n in tqdm.trange(N):\n",
    "        idxs = np.random.choice(K, size=K, replace=False)\n",
    "        hvs_temp = []\n",
    "        costs_temp = []\n",
    "        for k in range(1, K + 1):\n",
    "            temp2 = temp.iloc[idxs[:k]]\n",
    "            hv = compute_hv(temp2)\n",
    "            hvs_temp.append(hv)\n",
    "            costs_temp.append(\n",
    "                temp.st_worker_cost.iloc[idxs[k-1]] * ST_WORKER_TIME_DISCOUNT +\n",
    "                SCRIPT_SETUP_OVERHEAD_TIME / 60. / 60. *\n",
    "                instance_info(temp.config_st_instance_type.iloc[idxs[k-1]]).cost_per_hour\n",
    "            )\n",
    "        hvs_total.append(hvs_temp)\n",
    "        costs_total.append(costs_temp)\n",
    "\n",
    "    data[scenario_label] = np.array(hvs_total)\n",
    "    costs[scenario_label] = np.array(costs_total).cumsum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc40dc08",
   "metadata": {},
   "source": [
    "## random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e0b084",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1bfe5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = c\n",
    "scenario_label = 'random'\n",
    "K = len(temp)\n",
    "\n",
    "compute_hvs_and_cost(temp, scenario_label, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7328f7",
   "metadata": {},
   "source": [
    "## only cheapest instance per GPU-type-and-number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f91c259",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_types = [\n",
    "        'ml.p3.2xlarge',\n",
    "        'ml.p2.xlarge',\n",
    "        'ml.p2.8xlarge',\n",
    "        'ml.g4dn.xlarge',\n",
    "        # 'ml.g4dn.2xlarge',\n",
    "        # 'ml.g4dn.4xlarge',\n",
    "        # 'ml.g4dn.8xlarge',\n",
    "        'ml.g4dn.12xlarge',\n",
    "        'ml.g5.xlarge',\n",
    "        # 'ml.g5.2xlarge',\n",
    "        # 'ml.g5.4xlarge',\n",
    "        # 'ml.g5.8xlarge',\n",
    "        'ml.g5.12xlarge',\n",
    "#         'ml.g5.24xlarge',\n",
    "    ]\n",
    "\n",
    "temp = c.query('config_st_instance_type in @instance_types')\n",
    "scenario_label = 'cheapest_instances'\n",
    "\n",
    "K = len(temp)\n",
    "\n",
    "\n",
    "compute_hvs_and_cost(temp, scenario_label, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3170502c",
   "metadata": {},
   "source": [
    "## deterministically determining largest possible batch_size using only 1 instance_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e5a8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_types = [\n",
    "        'ml.p3.2xlarge',\n",
    "        'ml.p2.xlarge',\n",
    "        'ml.p2.8xlarge',\n",
    "        'ml.g4dn.xlarge',\n",
    "        'ml.g4dn.2xlarge',\n",
    "        'ml.g4dn.4xlarge',\n",
    "        'ml.g4dn.8xlarge',\n",
    "        'ml.g4dn.12xlarge',\n",
    "        'ml.g5.xlarge',\n",
    "        'ml.g5.2xlarge',\n",
    "        'ml.g5.4xlarge',\n",
    "        'ml.g5.8xlarge',\n",
    "        'ml.g5.12xlarge',\n",
    "        'ml.g5.24xlarge',\n",
    "    ]\n",
    "\n",
    "\n",
    "temp = c.query('config_st_instance_type in @instance_types')\n",
    "temp = (\n",
    "    temp\n",
    "    .groupby('config_st_instance_type')\n",
    "    .apply(lambda group: group.loc[group['config_per_device_train_batch_size'] == group['config_per_device_train_batch_size'].max()])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "scenario_label = 'determine_batch_size_one_instance_type'\n",
    "\n",
    "K = len(temp)\n",
    "\n",
    "compute_hvs_and_cost(temp, scenario_label, K)\n",
    "\n",
    "# add the cost of running two runs for each instance_type\n",
    "instance_type = 'ml.g5.xlarge'\n",
    "costs[scenario_label] += (\n",
    "    2 * ST_WORKER_TIME_DISCOUNT * \n",
    "    temp.groupby('config_st_instance_type').st_worker_cost.min()[instance_type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fc331d",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_types = [\n",
    "        'ml.p3.2xlarge',\n",
    "        'ml.p2.xlarge',\n",
    "        'ml.p2.8xlarge',\n",
    "        'ml.g4dn.xlarge',\n",
    "        # 'ml.g4dn.2xlarge',\n",
    "        # 'ml.g4dn.4xlarge',\n",
    "        # 'ml.g4dn.8xlarge',\n",
    "        'ml.g4dn.12xlarge',\n",
    "        'ml.g5.xlarge',\n",
    "        # 'ml.g5.2xlarge',\n",
    "        # 'ml.g5.4xlarge',\n",
    "        # 'ml.g5.8xlarge',\n",
    "        'ml.g5.12xlarge',\n",
    "        # 'ml.g5.24xlarge',\n",
    "    ]\n",
    "\n",
    "\n",
    "temp = c.query('config_st_instance_type in @instance_types')\n",
    "temp = (\n",
    "    temp\n",
    "    .groupby('config_st_instance_type')\n",
    "    .apply(lambda group: group.loc[group['config_per_device_train_batch_size'] == group['config_per_device_train_batch_size'].max()])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "scenario_label = 'determine_batch_size_one_instance_type_cheapest_instances'\n",
    "\n",
    "K = len(temp)\n",
    "\n",
    "\n",
    "compute_hvs_and_cost(temp, scenario_label, K)\n",
    "\n",
    "# add the cost of running two runs for each instance_type\n",
    "instance_type = 'ml.g5.xlarge'\n",
    "costs[scenario_label] += 2 * temp.groupby('config_st_instance_type').st_worker_cost.min()[instance_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23706124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e554b575",
   "metadata": {},
   "source": [
    "## Reuseable code for loading Syne Tune Blackbox runs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974b3f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6e25df",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket = 'mnemosyne-team-bucket'\n",
    "syne_tune_folder = 'AdamG/syne-tune-copy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4d5732",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_boto3 = boto3.client('sagemaker')\n",
    "s3 = boto3.resource('s3')\n",
    "my_bucket = s3.Bucket(s3_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7bc14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_with_syne_tune_results(scenario_label, runs):\n",
    "    temp_data = []\n",
    "    temp_cost = []\n",
    "    for run in runs:\n",
    "#         print(run)\n",
    "        dfff = syne_tune.experiments.load_experiment(tuner_name=run).results\n",
    "\n",
    "        inputs_unnorm_vec = np.stack([dfff['training-runtime-per-sample'], dfff['training-cost-per-sample']], axis=-1)\n",
    "        inputs_norm_vec = normalizer.transform(inputs_unnorm_vec)\n",
    "        temp_data.append([\n",
    "            pygmo.hypervolume(inputs_norm_vec[:i+1].tolist()).compute(ref_point) \n",
    "            for i in range(len(dfff))\n",
    "        ])\n",
    "\n",
    "        temp_cost.append(np.cumsum(dfff.st_worker_cost))\n",
    "\n",
    "    min_index = min(len(v) for v in temp_data)\n",
    "    data[scenario_label] = np.zeros((len(temp_data), min_index))\n",
    "    costs[scenario_label] = np.zeros((len(temp_data), min_index))\n",
    "    for i in range(len(temp_data)):\n",
    "        data[scenario_label][i, :] = temp_data[i][:min_index]\n",
    "        costs[scenario_label][i, :] = temp_cost[i][:min_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bfa82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tuner_names_and_features(experiment_names):\n",
    "    tuner_names_and_features = []\n",
    "    for experiment_job_name in experiment_names:\n",
    "        experiment_job_name_without_hash = '-'.join(experiment_job_name.split('-')[:-1])\n",
    "        tuners_names = set()\n",
    "        for my_bucket_object in my_bucket.objects.filter(\n",
    "            Prefix=f'{syne_tune_folder}/{experiment_job_name_without_hash}/'\n",
    "        ):\n",
    "            tuner_name = my_bucket_object.key.split('/')[-2]\n",
    "            if tuner_name.startswith('simulated-tabular-backend-'):\n",
    "                tuners_names.add(tuner_name)\n",
    "        \n",
    "        features_file_name = f'{syne_tune_folder}/{experiment_job_name_without_hash}/features.txt'\n",
    "        features_string = None\n",
    "        for obj in my_bucket.objects.filter(Prefix=features_file_name):\n",
    "            key = obj.key\n",
    "            features_string = obj.get()['Body'].read().decode('utf-8')\n",
    "            break\n",
    "        if not features_string:\n",
    "            print('Loading features from boto3')\n",
    "            job_details = sagemaker_boto3.describe_training_job(TrainingJobName=experiment_job_name)\n",
    "            features_string = job_details['HyperParameters']['features'].replace('\"', '')\n",
    "            temp_path = f\"/tmp/{''.join(random.choices(string.ascii_uppercase + string.digits, k=10))}.txt\"\n",
    "            with open(temp_path, \"w\") as f:\n",
    "                f.write(features_string)\n",
    "            \n",
    "            s3_path = f\"s3://{s3_bucket}/{features_file_name}\"\n",
    "            command = ['aws', 's3', 'cp', temp_path, s3_path, '--profile', 'mnemosyne']\n",
    "            print(' '.join(command))\n",
    "            subprocess.run(command)\n",
    "\n",
    "        features = features_string.split(' ')\n",
    "\n",
    "        tuner_names_and_features.append((features, tuners_names))\n",
    "    return tuner_names_and_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d5d6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def iqm(data, quantiles):\n",
    "    \"\"\" interquantile mean along axis=0 \"\"\"\n",
    "    axis = 0\n",
    "    k0 = math.floor(quantiles[0] * data.shape[axis])\n",
    "    k1 = math.ceil(quantiles[1] * data.shape[axis])\n",
    "    assert k1 > k0\n",
    "    data = np.partition(data, k0, axis=axis)\n",
    "    data[k0+1:] = np.partition(data[k0+1:], k1-k0-1, axis=axis)\n",
    "    return np.mean(data[k0:k1+1], axis=axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce8ae35",
   "metadata": {},
   "source": [
    "## Load automated sweeps from S3 with deterministic_transform=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a51ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "if INSTANCE_STARTUP_OVERHEAD_TIME == 0 and SCRIPT_SETUP_OVERHEAD_TIME == 360 and ST_WORKER_TIME_DISCOUNT == 1.0:\n",
    "    experiment_names = ['accurate-tamarin-Y3os', 'quantum-earwig-kY2f', 'classic-manatee-2Aam', \n",
    "     'remarkable-petrel-zdX4', 'groovy-woodlouse-xn4H', 'evasive-otter-vWyT', \n",
    "     'flawless-sponge-hTUL', 'idealistic-leopard-axOh', 'lavender-termite-DIR8', \n",
    "     'crystal-mole-p2Tt']\n",
    "elif INSTANCE_STARTUP_OVERHEAD_TIME == 0 and SCRIPT_SETUP_OVERHEAD_TIME == 410 and ST_WORKER_TIME_DISCOUNT == 0.1:\n",
    "    experiment_names = ['discreet-bird-4a4z', 'incredible-perch-zED3', 'cerulean-dinosaur-XsdV', \n",
    "                        'phenomenal-booby-x9al', 'invaluable-scallop-BCZb', 'blazing-okapi-1JHE', \n",
    "                        'humongous-lizard-Qgon', 'spirited-malkoha-SKgz', 'hungry-lemming-o7MP', \n",
    "                        'imaginary-kudu-caEr']\n",
    "elif INSTANCE_STARTUP_OVERHEAD_TIME == 0 and SCRIPT_SETUP_OVERHEAD_TIME == 0 and ST_WORKER_TIME_DISCOUNT == 0.1:\n",
    "    experiment_names = ['tireless-sidewinder-wqkF', 'overjoyed-spoonbill-4WIO', 'dexterous-cow-IoJp', \n",
    "                        'robust-jacamar-Il1a', 'dexterous-chachalaca-S36c', 'amorphous-hamster-D3ST', \n",
    "                        'observant-beluga-BRPe', 'tricky-quail-P3ef', 'rational-porcupine-p2sW', \n",
    "                        'unyielding-kakapo-JmBr']\n",
    "else:\n",
    "    raise Exception()\n",
    "    \n",
    "experiment_names_without_hash = ['-'.join(x.split('-')[:-1]) for x in experiment_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8000d951",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in experiment_names_without_hash:\n",
    "    !echo aws s3 cp ... s3://{s3_bucket}/{syne_tune_folder}/{i}/ ~/syne-tune/.\n",
    "    !aws s3 cp --recursive --quiet s3://{s3_bucket}/{syne_tune_folder}/{i}/ ~/syne-tune/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752aa07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_names_and_features = get_tuner_names_and_features(experiment_names)\n",
    "for features, tuner_names in tqdm.tqdm(tuner_names_and_features):\n",
    "    scenario_label = ','.join(f.replace('config_', '') for f in features)\n",
    "    runs = tuner_names\n",
    "\n",
    "    populate_with_syne_tune_results(scenario_label, runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb58dadd",
   "metadata": {},
   "source": [
    "## sweeps with deterministic_transform=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73811d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if INSTANCE_STARTUP_OVERHEAD_TIME == 0 and SCRIPT_SETUP_OVERHEAD_TIME == 360 and ST_WORKER_TIME_DISCOUNT == 1.0:\n",
    "    experiment_names = ['steady-robin-EWOG', 'encouraging-stoat-gP0a', 'observant-guppy-R5da', \n",
    "                        'tunneling-toucanet-eH7R', 'qualified-crocodile-mrbl', 'daring-bloodhound-3qMk', \n",
    "                        'industrious-dolphin-5C6f', 'original-shark-3MYw', 'white-scorpion-w4KW', \n",
    "                        'agate-nautilus-a3pH']\n",
    "elif INSTANCE_STARTUP_OVERHEAD_TIME == 0 and SCRIPT_SETUP_OVERHEAD_TIME == 410 and ST_WORKER_TIME_DISCOUNT == 0.1:\n",
    "    experiment_names = None\n",
    "elif INSTANCE_STARTUP_OVERHEAD_TIME == 0 and SCRIPT_SETUP_OVERHEAD_TIME == 0 and ST_WORKER_TIME_DISCOUNT == 0.1:\n",
    "    experiment_names = ['tough-quail-LXeR', 'hasty-potoo-nOkm', 'heavenly-sparrow-sddJ', \n",
    "                        'outstanding-chihuahua-d0ro', 'secret-nightingale-sdMC', 'blue-alpaca-jVFO', \n",
    "                        'aquatic-butterfly-kQjp', 'positive-pony-kWoo', 'towering-rook-Hct5', \n",
    "                        'marigold-loon-mUD6', \n",
    "                        'nano-mushroom-hLZJ', \n",
    "                        'unique-moth-pqfs'\n",
    "                       ]\n",
    "else:\n",
    "    raise Exception()\n",
    "    \n",
    "experiment_names_without_hash = ['-'.join(x.split('-')[:-1]) for x in experiment_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f832fc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in experiment_names_without_hash:\n",
    "    !echo aws s3 cp ... s3://{s3_bucket}/{syne_tune_folder}/{i}/ ~/syne-tune/.\n",
    "    !aws s3 cp --recursive --quiet s3://{s3_bucket}/{syne_tune_folder}/{i}/ ~/syne-tune/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff0ea14",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_names_and_features = get_tuner_names_and_features(experiment_names)\n",
    "for features, tuner_names in tqdm.tqdm(tuner_names_and_features):\n",
    "    scenario_label = 'det:'+','.join(f.replace('config_', '') for f in features)\n",
    "    runs = tuner_names\n",
    "\n",
    "    populate_with_syne_tune_results(scenario_label, runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3455bfd4",
   "metadata": {},
   "source": [
    "## random ST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6679f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if INSTANCE_STARTUP_OVERHEAD_TIME == 0 and SCRIPT_SETUP_OVERHEAD_TIME == 360 and ST_WORKER_TIME_DISCOUNT == 1.0:\n",
    "    experiment_names = None\n",
    "elif INSTANCE_STARTUP_OVERHEAD_TIME == 0 and SCRIPT_SETUP_OVERHEAD_TIME == 410 and ST_WORKER_TIME_DISCOUNT == 0.1:\n",
    "    experiment_names = None\n",
    "elif INSTANCE_STARTUP_OVERHEAD_TIME == 0 and SCRIPT_SETUP_OVERHEAD_TIME == 0 and ST_WORKER_TIME_DISCOUNT == 0.1:\n",
    "    experiment_names = ['versed-mammoth-KgK9']\n",
    "else:\n",
    "    raise Exception()\n",
    "    \n",
    "experiment_names_without_hash = ['-'.join(x.split('-')[:-1]) for x in experiment_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe0ee86",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in experiment_names_without_hash:\n",
    "    !echo aws s3 cp ... s3://{s3_bucket}/{syne_tune_folder}/{i}/ ~/syne-tune/.\n",
    "    !aws s3 cp --recursive --quiet s3://{s3_bucket}/{syne_tune_folder}/{i}/ ~/syne-tune/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef0816f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_names_and_features = get_tuner_names_and_features(experiment_names)\n",
    "for features, tuner_names in tqdm.tqdm(tuner_names_and_features):\n",
    "    scenario_label = 'random-ST'\n",
    "    runs = tuner_names\n",
    "\n",
    "    populate_with_syne_tune_results(scenario_label, runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ed0c4d",
   "metadata": {},
   "source": [
    "## Plotting settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a554394b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which quantiles to plot\n",
    "k = 0.45\n",
    "quantile_values_parameter = [0.5-k, 0.5+k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f42d12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colors\n",
    "colormap_name = 'Set1'\n",
    "cmap_colors = plt.get_cmap(colormap_name).colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9a0035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all combinations\n",
    "list(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40093c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subselect the traces to plot to avoid mess\n",
    "traces_to_plot = [\n",
    "    'random', \n",
    "#     'random-ST',\n",
    "#     'cheapest_instances', \n",
    "#     'determine_batch_size_one_instance_type', \n",
    "# #     'determine_batch_size_one_instance_type_cheapest_instances', \n",
    "#     'det', \n",
    "    \n",
    "    'det:st_instance_type,per_device_train_batch_size,dataloader_num_workers', \n",
    "#     'det:st_instance_type,GPUMemory/batch_size,dataloader_num_workers', \n",
    "    'det:st_instance_type,per_device_train_batch_size,dataloader_num_workers,GPUFP32TFLOPS', \n",
    "#     'det:st_instance_type,GPUMemory/batch_size,dataloader_num_workers,GPUFP32TFLOPS', \n",
    "    'det:st_instance_type,per_device_train_batch_size,dataloader_num_workers,GPUFP32TFLOPS*num_gpu', \n",
    "#     'det:st_instance_type,GPUMemory/batch_size,dataloader_num_workers,GPUFP32TFLOPS*num_gpu', \n",
    "#     'det:instance_type_family,num_cpu,per_device_train_batch_size,dataloader_num_workers', \n",
    "#     'det:instance_type_family,cost_per_hour,per_device_train_batch_size,dataloader_num_workers', \n",
    "#     'det:instance_type_family,num_cpu,GPUMemory/batch_size,dataloader_num_workers', \n",
    "#     'det:instance_type_family,cost_per_hour,GPUMemory/batch_size,dataloader_num_workers'\n",
    "    \n",
    "#     'det:instance_type_family,per_device_train_batch_size,dataloader_num_workers',\n",
    "#     'det:instance_type_family,GPUMemory/batch_size,dataloader_num_workers',\n",
    "#     'det:instance_type_family,per_device_train_batch_size,dataloader_num_workers,GPUFP32TFLOPS',\n",
    "#     'det:instance_type_family,GPUMemory/batch_size,dataloader_num_workers,GPUFP32TFLOPS'\n",
    "#     'det:instance_type_family,num_gpu,per_device_train_batch_size,dataloader_num_workers',\n",
    "    \n",
    "    'st_instance_type,per_device_train_batch_size,dataloader_num_workers', \n",
    "#     'st_instance_type,GPUMemory/batch_size,dataloader_num_workers', \n",
    "    'st_instance_type,per_device_train_batch_size,dataloader_num_workers,GPUFP32TFLOPS', \n",
    "# #     'st_instance_type,GPUMemory/batch_size,dataloader_num_workers,GPUFP32TFLOPS', \n",
    "# #     'st_instance_type,per_device_train_batch_size,dataloader_num_workers,GPUFP32TFLOPS*num_gpu', \n",
    "# #     'st_instance_type,GPUMemory/batch_size,dataloader_num_workers,GPUFP32TFLOPS*num_gpu', \n",
    "#     'instance_type_family,num_cpu,per_device_train_batch_size,dataloader_num_workers', \n",
    "# #     'instance_type_family,cost_per_hour,per_device_train_batch_size,dataloader_num_workers', \n",
    "# #     'instance_type_family,num_cpu,GPUMemory/batch_size,dataloader_num_workers', \n",
    "# #     'instance_type_family,cost_per_hour,GPUMemory/batch_size,dataloader_num_workers'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55161a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want you can set custom plotting settings\n",
    "\n",
    "# plotting_settings = {\n",
    "#     'random': dict(color='b', label='random_search'),\n",
    "# #     'random_b': dict(color='r', label='random_search_without_failed'),\n",
    "# #     'cheapest_instances': dict(color='r', label='+ cheapest_instances'),\n",
    "# #     'cheapest_instances_b': dict(color='m', label='+ cheapest_instances_without_failed'),\n",
    "# #     'determine_batch_size': dict(color='g', label='+ determine_max_batch_size'),\n",
    "# #     'determine_batch_size_one_instance_type': dict(color='c', label='+ determine_max_batch_size'),\n",
    "# #     'determine_batch_size_one_instance_type_cheapest_instances': dict(color='g', label='+ subset_of_instances'),\n",
    "# #     'mobo': dict(color='k', label='MOBO'),\n",
    "#     'rf_mobo': dict(color='k', style='dashed', label='MOBO +GPUFP32TFLOPS'),\n",
    "# #     'config_st_instance_type,config_per_device_train_batch_size,config_dataloader_num_workers,GPUFP32TFLOPS*num_gpu': \n",
    "# #     dict(color='k', style='dashed', label='config_st_instance_type,config_per_device_train_batch_size,config_dataloader_num_workers,GPUFP32TFLOPS*num_gpu'),\n",
    "#     'config_st_instance_type,GPUMemory/batch_size,config_dataloader_num_workers,GPUFP32TFLOPS*num_gpu': \n",
    "#     dict(color='k', style='dashed', label='config_st_instance_type,GPUMemory/batch_size,config_dataloader_num_workers,GPUFP32TFLOPS*num_gpu'),\n",
    "# #     'rs_st': dict(color='m', label='RS-ST'),\n",
    "# }\n",
    "\n",
    "\n",
    "# Or you can use automated defaults\n",
    "\n",
    "plotting_settings = {label: dict(color=cmap_colors[i%len(cmap_colors)], label=label)\n",
    "                     for i, label in enumerate([i for i in data.keys() if i in traces_to_plot])\n",
    "                     }\n",
    "\n",
    "defaults = dict(color=None, style=None, label=None)\n",
    "plotting_settings = {k: {kd: v[kd] if kd in v else vd \n",
    "                         for kd, vd in defaults.items()} \n",
    "                     for k, v in plotting_settings.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f67f12",
   "metadata": {},
   "source": [
    "## Plot: vs number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0dae55",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 8), facecolor='white')\n",
    "\n",
    "for k, v in plotting_settings.items():\n",
    "    ax.plot(true_hv-np.mean(data[k], axis=0), color=v['color'], label=v['label'])\n",
    "\n",
    "    quantiles = np.quantile(data[k], quantile_values_parameter, axis=0)\n",
    "    ax.fill_between(x=np.arange(len(quantiles[0])), y1=true_hv-quantiles[0], y2=true_hv-quantiles[1], \n",
    "                    alpha=0.1, color=v['color'])\n",
    "\n",
    "# ax.set_xlim(0, 100)\n",
    "# ax.set_ylim(0, 10000)\n",
    "\n",
    "ax.set_xlabel(\"Number of random samples\")\n",
    "ax.set_ylabel(\"Hypervolume error\")\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00af86c",
   "metadata": {},
   "source": [
    "## Plot: vs cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcce4a9",
   "metadata": {},
   "source": [
    "The results of some methods (some traces on the plots, e.g., `cheapest_instances`, `determine_batch_size_one_instance_type`, `determine_batch_size_one_instance_type_cheapest_instances`) are not asymptotically exact because some of the Pareto-front instances are not included in the subset of configurations considered by those methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8956332",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Create traces\n",
    "fig = go.Figure()\n",
    "\n",
    "for k, v in plotting_settings.items():\n",
    "    interp_xs_length = 2000\n",
    "    data_interpolated = np.zeros((len(data[k]), interp_xs_length))\n",
    "    interp_xs = np.linspace(0, costs[k].max(), interp_xs_length)\n",
    "    for n in range(data[k].shape[0]):\n",
    "        data_interpolated[n, :] = np.interp(interp_xs, xp=costs[k][n, :], fp=data[k][n, :])\n",
    "    \n",
    "    fig.add_trace(go.Scatter(x=interp_xs, y=true_hv-iqm(data_interpolated, quantiles=quantile_values_parameter),\n",
    "                mode='lines',\n",
    "                name=v['label']))\n",
    "\n",
    "    quantiles = np.quantile(data_interpolated, quantile_values_parameter, axis=0)\n",
    "#     ax.fill_between(x=interp_xs, y1=quantiles[0], y2=quantiles[1], \n",
    "#                     alpha=0.1, color=v['color'])\n",
    "\n",
    "\n",
    "fig.update_yaxes(type=\"log\", range=(-4, 0))\n",
    "\n",
    "fig.update_layout(\n",
    "    autosize=True,\n",
    "    width=1000,\n",
    "    height=800,\n",
    "    xaxis_title=\"Cost in $\",\n",
    "    yaxis_title=\"Hypervolume error\",\n",
    "    legend=dict(\n",
    "        yanchor=\"top\",\n",
    "        y=-0.1,\n",
    "        xanchor=\"left\",\n",
    "        x=0.01\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c911d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8), facecolor='white')\n",
    "\n",
    "for k, v in plotting_settings.items():\n",
    "    interp_xs_length = 2000\n",
    "    data_interpolated = np.zeros((len(data[k]), interp_xs_length))\n",
    "    interp_xs = np.linspace(0, costs[k].max(), interp_xs_length)\n",
    "    for n in range(data[k].shape[0]):\n",
    "        data_interpolated[n, :] = np.interp(interp_xs, xp=costs[k][n, :], fp=data[k][n, :])\n",
    "    \n",
    "#     ax.plot(interp_xs, true_hv-np.mean(data_interpolated, axis=0), color=v['color'], label=v['label'], linestyle=v['style'])\n",
    "    ax.plot(interp_xs, true_hv-iqm(data_interpolated, quantile_values_parameter), color=v['color'], label=v['label'], linestyle=v['style'])\n",
    "\n",
    "    quantiles = np.quantile(data_interpolated, quantile_values_parameter, axis=0)\n",
    "    ax.fill_between(x=interp_xs, y1=true_hv-quantiles[0], y2=true_hv-quantiles[1], \n",
    "                    alpha=0.1, color=v['color'])\n",
    "\n",
    "# ax.set_xlim(0, 40)\n",
    "\n",
    "# ax.set_xscale('log', base=10) \n",
    "# ax.set_xlim(0.5, max(c.max() for c in costs.values()))\n",
    "plt.yscale('log',base=10)\n",
    "# ax.set_xlim(0., 40)\n",
    "ax.set_ylim(1e-6, 1.)\n",
    "\n",
    "ax.set_xlabel(\"Cost in $\")\n",
    "ax.set_ylabel(\"Hypervolume error\")\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f657de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8), facecolor='white')\n",
    "\n",
    "for k, v in plotting_settings.items():\n",
    "    interp_xs_length = 2000\n",
    "    data_interpolated = np.zeros((len(data[k]), interp_xs_length))\n",
    "    interp_xs = np.linspace(0, costs[k].max(), interp_xs_length)\n",
    "    for n in range(data[k].shape[0]):\n",
    "        data_interpolated[n, :] = np.interp(interp_xs, xp=costs[k][n, :], fp=data[k][n, :])\n",
    "    \n",
    "    ax.plot(interp_xs, 1.-np.mean(data_interpolated, axis=0), color=v['color'], label=v['label'], linestyle=v['style'])\n",
    "\n",
    "    quantiles = np.quantile(data_interpolated, quantile_values_parameter, axis=0)\n",
    "    ax.fill_between(x=interp_xs, y1=1.-quantiles[0], y2=1.-quantiles[1], \n",
    "                    alpha=0.1, color=v['color'])\n",
    "\n",
    "# ax.set_xlim(0, 40)\n",
    "\n",
    "# plt.axvline(x=costs['determine_batch_size'].min(), color='b')\n",
    "plt.axhline(y=1.-true_hv, color='k', label='Ground truth hypervolume')\n",
    "\n",
    "# ax.set_xscale('log', base=10) \n",
    "# ax.set_xlim(0.5, max(c.max() for c in costs.values()))\n",
    "# plt.yscale('log',base=10) \n",
    "ax.set_xlim(0., 6)\n",
    "ax.set_ylim(1e-2, 0.1)\n",
    "\n",
    "ax.set_xlabel(\"Cost in $\")\n",
    "ax.set_ylabel(\"1 - hypervolume (NOT ERROR!)\")\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7389bdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda49d87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7551e11b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
